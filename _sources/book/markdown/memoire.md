# Mémoire digitale: les enjeux de l'archive numérique et du Big Data

Le mot archive souffre d'un imaginaire collectif qui voudrait que celle-ci soit la panacée des librairies, conservateur ou bureaucrate. On imagine des piles de documents entreposés dans des boîtes en carton, sur les étagères d’un corridor poussiéreux, noyés sous des systèmes organisationnels. Inaccessible, rigide et opaque à l’étranger qui ne possède pas les logiques qu’elle habite. 

Dans une époque piloté par la culture du data-driven, le mot numérique quant à lui est la promesse de fluidité (*streaming*), de légèreté (*Cloud*), de transparence (*Window*), et de renouveau. 

L’ « archive numérique » c’est quoi alors ? Faire du neuf avec du vieux ? Si ces deux mots peuvent coexister ensemble, c’est notamment grâce à la notion d’information. C’est l’information qu’on archive, qu’on numérise, qu’on souhaite rendre plus légère, plus rapide et transparente. Comprendre les enjeux autour de l’archive numérique, c’est donc avant tout une histoire d’information.   

Conscient qu’il existe de très nombreux pans de recherches et de débats possible autour de ces enjeux, ici je veux m’attarder sur les enjeux structurels en termes d’accessibilité, de préservation, et de contrôle. Ouvrir des voies de réflexion, souligner leurs ressemblances pour mettre en lumière leurs différences. Mais pour se faire, il faut revenir et définir les notions d’archive, d’information et de numérique. C’est ce que je me propose de faire avant d’aborder plus amplement les enjeux structurels. Mon approche jongle entre théorie de la communication, étude des médias et humanités numériques. Sans faire dans le techno-déterminisme, je veux également montrer qu'il est important d'étudier comment les technologies modifient et redéfinissent notre rapport au monde et à l’information.

## Une architecture de mémoire

Toute mémoire possède une architecture, car toute mémoire est construite. Parler de mémoire, c'est parler de ce qui nous définit en tant qu'individu, mais aussi en tant qu'être humain. Aujourd'hui l'homme est en capacité de construire et d'organiser son propre palais mnémonique. Bibliothèques et musées sont construits pour conserver et valoriser notre patrimoine. Et depuis, avec l'omniprésente de nos écosystèmes informatiques, les bases de données viennent s'ajouter à cette liste d'édifices. Elles y stockent nos souvenirs, nos savoirs, ainsi que nos échanges informationnels.   

Mais il serait réducteur de penser que la mémoire n'est qu'un lieu de stockage, et qu'elle ne dispose que d'une fonction, celle de rétention. La fonction de la mémoire est avant tout :

> *«Celle de l'élection, du prélèvement, du rappel et du retour d'un unique élément au sein de ce qui a été stocké en bloc.[...] La mémoire est d'abord une sélection de ce qui est à oublier, ensuite seulement une rétention de ce qu'on entend mettre à l'écart de l'emprise de l'oubli qui la fonde*» {cite:p}`quignard_1993`


La mémoire est donc ce qui vient donner sens, organiser et architecturer un contenu, à ne pas confondre avec le contenant. La mémoire, c'est ce qui vient structurer de l'information, la rendre intègre, accessible et lui donne par conséquence sens. Car la donnée en elle-même n'est rien, si on ne sait pas l'interroger. Dans son livre intitulé *Le Nom sur le bout de la langue*, Pascal Guignard se joue de cette interrogation qui parfois dysfonctionne. Il relate cet événement que tout individu à déjà expérimenté qui consiste à perdre le mot qu'on s'apprêtait à utiliser, et montre ainsi que toute architecture de mémoire présente des failles.

> *«Cette expérience du mot qu'on sait et dont on est sevré, est l'expérience où l'oubli de l'humanité qui est en nous agresse. Où le caractère fortuit de nos pensées, où la nature fragile de notre identité, où la matière involontaire de notre mémoire et son étoffe exclusivement linguistique se touchent avec les doigts. C'est l'expérience où nos limites et notre mort se confondent pour la première fois. C'est la détresse propre au langage humain. C'est la détresse devant ce qui est acquis. Le nom sur le bout de la langue nous rappelle que le langage n'est pas en nous un acte réflexe. Que nous ne sommes pas des bêtes qui parlent comme elles voient»* {cite:p}`quignard_1993`

Quand est-il de cette expérience de l'oubli pour l'individu du XXI{sup}`ème`siècle, capable d'étendre ces moyens mnémoniques par le biais de prothèses numériques ? Quand est-il de la mémoire à l'heure des données massives ?

Quand on additionne l'ensemble de ces prothèses riches en bits d'informations, on obtient l'édifice le plus important jamais construit par l'homme, en termes de rétention, mais aussi de défaillance. Cette nouvelle architecture, que l'on nomme communément le Big Data, est une prothèse divine à porter de *clic*. Mais comment se fait cette élection d'information ? Comment s'organise-t-elle? Comment l'interroger ? Et quelles sont ses failles ?


### L'archive, une question de pouvoir

Parler de mémoire, c'est faire le parallèle avec l'archive. L'archive comme la mémoire c'est l'élection, le prélèvement, et le rappel de cette information dans le présent {cite:p}`quignard_1993`. Pour Arlette Farge {cite:p}`farge_1989` « le travail en archive oblige forcément à des opérations de tri de séparation des documents [...] les mots sont porteurs de présent, éléments de reconnaissance et de distinction du temps d’où ils sont issus ». Parler d'archive, c'est également faire le lien avec la technique. Toutes les révolutions techniques, comme la presse de Gutenberg puis la presse rotative, le cinéma des Frères Lumières, le disque vinyle, la photographie sur pellicule et, aujourd'hui, nos entrepôts de données. On eut pour conséquences, au-delà de la diffusion de l'information et de son stockage, sa production organisée et répétée dans une quantité toujours plus grande. Au début du XX{sup}`ème`siècle, l'archiviste Hilary Jenkinson soulevait déjà ce problème :

> «*Là existe un réel danger que l'historien du futur, sans parler de l'archiviste, soit enterré sous la masse de ses autorités manuscrites [sic] ; ou alternativement que pour faire face aux accumulations des mesures puissent être prises qu'aucun archiviste ne pourrait approuver*» {cite:p}`jenkinson_1937`

Car, là, se trouve déjà le premier défi que représente l'archive à l'époque du Big Data : la quantité. Comment effectuer, c'est opération de trie dont Arlette Farge {cite:p}`farge_1989` nous parle, quand la masse d'information aujourd'hui est de l'ordre du pétabytes? Un pétabytes, c'est l'équivalent d'un milliard de livre{cite:p}`ionos`. Aujourd'hui des entreprises comme Facebook étendard du Big Data, archive par jour 100 pétabytes de photo et vidéo{cite:p}`lefigaro_2012`. Facebook comme toutes les entreprises et institutions qui participent à l'archivage de notre mémoire collective et personnelle sont également nos nouveaux lieux de pouvoir. Car, parler d'archive, c'est également faire le parallèle entre négociation, contrôle, savoir et pouvoir.

Avec l'archive, l'élection de l'information s'accompagne implicitement de critères de valeurs, car ces opérations sont toujours effectuées par des groupes d'individus. Pour le courant poststructuralisme, l'archive à toujours été une chose dynamique et productrice de savoirs. Ce qu'Annie Ring nomme les « opérations herméneutiques » {cite:p}`ring_2014` sont toujours des modes d'accès ou de fermeture à celui-ci. L'archive est toujours le théâtre de bataille autour de la « connaissance » et de l'accès à l'information. Dans un ouvrage fondateur *Le Mal d'Archive* {cite:p}`derrida_1995`, Jacques Derrida retrace l'étymologie du mot archive. *Arché* en grec, signifie à la fois le commencement et le commandement. *Arkheion* quant à lui, désigne la maison où les anciens magistrats (les archontes) entreposaient les documents dont ils étaient les gardiens et dépositaires.

L'archive c'est donc « la loi ce qui peut-être dit »{cite:p}`foucault_1969`, elle reste en permanence dynamique et vulnérable aux transformations. Comme chaque loi, elle peut-être réinterprétée ou bien détruite. L'archivage à l'ère des données massives suppose le même jeu de pouvoir et d'exclusion. Quand, en 1969, Michel Foucault écrit *L'archéologie du savoir* {cite:p}`foucault_1969`, il souligne le fait qu'il est possible de trouver, entre les « pages blanches » de l'archive, une forme d'historicité et de corrélation, entre les « maîtres » de l'archivage et les exclus de l'histoire. 

Cette logique d'exclusion des archives est souvent expliquée par les penseuses féministes françaises, comme Hélène Cixous, qui a fait don de l'ensemble de ses manuscrits à la Bibliothèque Nationale de France. Elle constate que l'archive revêt le plus souvent un voile de masculinité, notamment à travers un système patriarcal omniprésent du langage, du discours et de l'histoire. De nombreux archivistes ont également pointé l'exclusion des femmes de l'archive historique, et la difficulté de localiser leurs subjectivités dans les sources d'archives, souvent obscurcit :

> «*Par des événements de grande ampleur jugés plus importants que ceux qui encadrent leur vie, et par des grands récits qui peuvent toucher à des contextes importants pour eux.*»{cite:p}`burton_2008`

Ces courants de pensées soulignent que l'archive est toujours une question de sélection, d'exclusion, de silence et de subjectivité. Et le Big Data n'échappe pas à ces problématiques... La seule différence est qu'aujourd'hui la chaîne de traitement qui s'applique à l'archivage n'est plus la même, et que les archivistes n'ont plus rien d'académique. 

### Le protocole, entre information et savoir

Pour comprendre le changement qui est en cours dans la façon dont nous structurons l'information, il faut avant tout revenir sur la notion d'information. La première distinction à faire est de séparer la notion d'information et de savoir. Toute information n'est pas productrice de savoir, dans le sens ou toute information n'apporte pas forcément de nouvelles connaissances. Il est intéressant de revenir étymologiquement sur les mots savoir et information. Savoir en latin est un mélange de *sapĕre* « avoir de la saveur » et de *sapiens* qui signifie sage, d'où l'expression avoir de la sagesse, être sage, c'est-à-dire comprendre {cite:p}`wiki_sapere`. Information dérivée du latin *informatus* veut dire donné forme la forme accusatif *informationem* signifiant contour, concept, idée {cite:p}`wiki_informatio` .

On retrouve donc derrière l'idée de savoir, la notion de compréhension, une condition de connaissance et de prise de conscience qui repose sur des faits. Derrière la notion d'information, on retrouve essentiellement l’idée de transmettre, de communiquer et donner forme à ses pensées. Les études qui s'intéressent au savoir et à l'information n'ont pas le même âge. L'étude du savoir remonte à l'Antiquité avec la philosophie grecque. L'étude de l'information, elle, est en revanche beaucoup plus récente. Il a fallu attendre le mouvement cybernétique en provenance des États-Unis avec l'explosion des télécommunications (télégraphes, radios, télévisions, téléphones) pour théoriser la transmission d'informations. Grâce au célèbre modèle de communication de Shannon et Weaver, qui sera ensuite actualisé par Norbert Wiener, père fondateur de la cybernétique.

```{figure} assets/modele-shannon-wiener.png
:width: 500px
:align: center
Modèle cybernétique de Norbert Wiener
```

Afin de transmettre de l'information il nous faut un émetteur, un récepteur, ainsi qu'un canal par lequel transite le message. Depuis que les êtres humains existent, ils ont toujours utilisé des canaux afin de transmettre des messages. Un canal qui est également appelé médium, concerne tout ce qui sert à enregistrer, à transmettre et/ou traiter de l'information, des discours, des images, des sons. Les peintures et gravures trouvées à l'intérieur de la grotte de Lascaux, un manuscrit médiéval, une photographie sur pellicule, un téléphone, un tweet, reposent tous sur des médiums. Le médium c'est donc ce qui est médial, dans le sens où celui-ci se trouve dans un entre deux, entre un émetteur et un récepteur. Même si le médium dans ce qui nous intéresse est fortement lié à sa dimension technique, Fritz Heider nous invite à étendre le médium au delà de la matière et d'introduire d'autres formes de médialité :

> «*Ce qui nous entoure, nous l’appréhendons toujours d’une manière ou d’une autre. Nous n’appréhendons pas que des choses qui entrent dans un contact immédiat avec notre épiderme, mais bien souvent, nous appréhendons une chose à travers une autre. Par exemple, c’est à travers l’éther que nous percevons des étoiles distantes, c’est par l’air que nous entendons le son d’une cloche, c’est grâce au baromètre que nous sommes informés de la pression atmosphérique, par le biais d’un mouvement expressif, nous appréhendons une psychologie, à travers les yeux de quelqu’un, on jette un regard “dans les tréfonds de son âme”, à la vue de la graphie de quelqu’un, nous saisissons sa pensée et ainsi de suite.*»{cite:p}`heider_2017`

Chaque message qui transite par le biais d'un médium est contraint par celui-ci. C'est l'une des raisons pour lesquelles le message subit toujours un encodage par son émetteur, puis un décodage par son récepteur. Pour écrire un livre, il faut apprendre le système de langage dans lequel on évolue, mais également se contraindre aux limites du support sur lequel on écrit (dimensions, textures, outils). Quand un message transite par le biais d'un médium, différents bruits peuvent intervenir. Le bruit sémantique, si l'encodeur ou/et le récepteur ne maîtrise pas ou mal le système de langage nécessaire pour écrire ou lire le message. Le bruit technique qui est introduit par le médium lui-même, comme un livre ou une peinture abîmée par le passage du temps.

Tous ces bruits viennent dégrader le message. C'est là qu'intervient la notion de perte d'informations, qui n'a donc rien à voir avec la notion de savoir. Cette perte d'informations est la corrélation qu'il y a entre le message envoyé par l'émetteur et le message reçu par le récepteur. On parle alors de quantité d'informations. Le savoir, c'est quand cette quantité d'information se transforme en signification pour le récepteur car celui-ci y trouve ou projette des signes. C'est la fameuse opération herméneutique. {cite:p}`ring_2014` 

Information et savoir, transmission et signification, forment ce qu'on appelle un média « des structures de communication socialement réalisées, dans lesquelles les structures incluent à la fois des formes technologiques et les protocoles qui leur sont associés, et dans lesquelles la communication est une pratique culturelle, la collocation ritualisée de différentes personnes sur une même carte mentale, partageant ou engagées dans des ontologies populaires de représentation »{cite:p}`gitelman_2008`. Le mot important à retenir dans cette définition des médias, faite par Lisa Gitelman, est le mot « protocole » car ce sont les protocoles qui structurent notre mémoire et notre manière de penser. D'où l'affirmation la plus célèbre de Friedrich Kittler, théoricien des médias, « Les médias déterminent notre situation » {cite:p}`kittler_2018`

### Digitaliser le papier, une organisation informationnelle


Digitaliser le papier est une expression qui se veut provocatrice, car évidemment ce n'est pas le papier qu'on digitalise mais l'information qu'il y contient. Aujourd'hui s'opère une dichotomie entre le papier et l'écran, comme si la présence de l'un ne pouvait se faire qu'en l'absence de l'autre. Il est intéressant au contraire de revenir sur ce que ces deux entités ont en commun afin de souligner leurs différences.

```{figure} assets/blog-hintz-eric-2018-12-10-chord-keyset-mouse.jpg
:align: center
Douglas Engelbart utilisant le jeu de touches d'accord à 5 boutons du NLS, un clavier QWERTY standard et une souris à 3 boutons, vers 1968. Catalogue NMAH # 2015.3073.11, don de SRI International
```

La papier est fortement lié à l'univers du document, et le document est fortement lié à la fiche. Un document peut contenir plusieurs fiches et une fiche peut être contenue dans un seul document. Tout le monde peut voir le lien sémantique et logique qu'il existe entre l'univers informatique que représente l'écran et l'univers plus analogique lié au papier. Le 9 décembre 1968, Douglas Engelbart présente Mother of all Demos, présentation dont l'héritage est toujours présent aujourd'hui {cite:p}`mother_of_all_demos`. Engelbart y pose les bases modernes de l'interaction homme-machine, l'organisation des fichiers et dossiers en lien cliquable, les commandes de base que tout utilisateur néophyte ou pas utilisent de manière chronique; le copier, le coller, la recherche par mot-clé. Avec la volonté d'augmenter les capacités humaines, il présente également la souris, le clavier, le mélange de texte et de vidéo sur un même écran, à une époque où Internet n'était qu'à ses balbutiements. Visionnaire, il crée l'ontophanie numérique moderne. 

Douglas Engelbart a eu l'intelligence de comprendre que ce nouveau média, qu'était l'informatique, disposait de protocoles qui demandaient un effort d'apprentissage considérable. Il existe plusieurs niveaux d'abstraction afin de manipuler le protocole informatique :


```{figure} assets/niveaux-langages.png
:align: center
:width: 400px
Les niveaux de languages informatique
```

- Le langage machine, c'est-à-dire le langage le plus bas niveau qui interagit directement avec la machine. Des informations sous forme binaire, elles gèrent les accès mémoire de la machine, ses registres etc...
- Le langage d'assemblage traduisant le langage machine en un langage mnémonique compréhensible par l'homme fait de chiffre et de lettre.
- Le langage de haut niveau reposant sur le langage d'assemblage (via des assembleurs) et qui permet d'offrir encore un plus haut niveau d'abstraction via des signes et formes arithmétiques.


Le logiciel est l'aboutissement de cette abstraction. Le génie de Douglas Engelbart a été de transposer des protocoles qui étaient familiaux pour tous, au sein du médium qu'est l'informatique. Cette forme de duplicité à permis l'émergence de l'informatique d'aujourd'hui. Et avec le numérique la domination des protocoles liés au monde haptique, que représente le papier est quasi totale. Quand on parle de corbeille, de e-ticket, de publication, de page web, nous ne faisons que convoquer les protocoles liés au papier et au livre. Parce que ces protocoles sont intrinsèquement présents dans la façon dont l'être humain pense et structure l'information.

>  «*L'information est comprise aujourd'hui en "morceaux" ou morceaux discrets en partie à cause de la manière dont le concept d'information réifie les propriétés des documents papier ; ils sont séparés et séparables, délimités et distincts.*» {cite:p}`nunberg_1996`

Le fichier offre la possibilité de penser en termes de granularité : on collecte, on sépare, on jette à la corbeille, on regroupe dans un dossier, lui-même étant le sous-dossier d'un dossier. Quand l'archive est le symbole du savoir par le fait d'opérations herméneutiques sur de l'information, la bibliothèque est le symbole de l'organisation de cette information. Les bibliothécaires ont le rôle d'organiser ce savoir, de le rendre accessible. Aujourd'hui avec l'informatique chaque utilisateur, à travers son ordinateur, se transforme à la fois en archiviste et bibliothécaire. 

> «*La tâche du bibliothécaire est de mettre à disposition, et les techniques les plus fréquemment développées concernent les modes de recherche des collections pour identifier les matériaux. Si les bibliothèques ont une fonction de préservation, elles se distinguent par leur fonction clé d'indexation. La bibliothèque est une machine à récupérer des informations*»{cite:p}`cubitt_2006`

Aujourd'hui, nous archivons et traitons donc l'information par le biais de nos prothèses numériques, bien que notre mémoire et notre structure de pensée restent foncièrement attachée à la culture du papier. C'est donc une période de transition non pas entre le papier et l'écran mais au sein de notre propre palais mnémonique, qui est notre cerveau. Celui-ci naviguant dans le monde spatio-temporel qu'est le numérique à besoin de balises auxquelles s'accrocher. Car, encore une fois, ce qui change ce n'est pas tant l'information que sa structuration. Pour Bruno Bachimont, on bascule d'une raison graphique à une raison computationnelle :

>  «*La raison graphique a produit la raison classificatoire, la raison computationnelle produit la pensée en réseau et le temps de la prévision. Pour une raison graphique, le réseau n'est pas une structure intelligible: le réseau, échappant à la synopsis spatiale du fait de sa complexité, est un labyrinthe où l'ont se perd. C'est une figure de l'irrationnel, et non une manière de penser le monde.*»  {cite:p}`bachimont_2004`

L'écriture a permis la création d'une pensée particulière, d'un ordre graphique, avec un ordonnancement et une hiérarchisation de la page, ainsi que la construction de sommaire, d'index et de liste {cite:p}`goody_1979`. Tout cela a permis de mettre de la distance au sein de sa pensée mais également de nourrir une forme d'abstraction et de mémorisation. Aujourd'hui, avec la raison computationnelle, le caractère spatial de l'écriture est effacé au profit du calcul car le réseau repose avant tout sur l'ubiquité. Le calcul permet de « réduire la complexité et de parcourir l'ensemble des possibles induit par le réseau » {cite:p}`bachimont_2004`.

### Méta-média: une digitalisation qui se veut discrète

La raison computationnelle nous impose donc la notion de calcul, et c'est là que réside la différence entre le monde analogique et numérique. Le terme digitalisation ou grammatisation {cite:p}`auroux_1995`, c'est le passage du monde analogique au monde digital. L'univers analogique c'est celui de la continuité, l'univers digital c'est celui de la discrétisation. Deux concepts fondamentaux qui mérite une analogie :

> «*La distinction technique de base entre les représentations analogiques (continues) et numériques (discrètes) est ici cruciale. Rouler sur une rampe est un mouvement continu, mais marcher jusqu'en bas des escaliers est une séquence de marches discrètes, vous pouvez donc compter le nombre de marches, mais pas le nombre de niveaux sur la rampe.*» {cite:p}`mitchell_1994`



```{figure} assets/numerisation.png
:align: center
Numérisation d'un signal analogique
```

La discrétisation c'est donc le passage de valeurs continues à un ensemble de valeurs discrètes. Un autre exercice de pensée consiste à imaginer l'ensemble des nombres qui puissent exister entre 0 et 10. D'un point de vue mathématique il existe un continuum infini de nombre comme `1|3|4.5|4.6|4.666|4.6665` - c'est le monde analogique. Dans le monde des valeurs discrète, il n'existe que `0|1|2|3|4|5|6|7|8|9|10` à l'instar de `A|B` il n'y pas de continuum entre ces valeurs - c'est le monde digital.

Maintenant prenons un autre exemple, mais cette fois-ci avec le média qu'est la photographie. Imaginons que l'on prenne une photographie argentique et avec une loupe grossissante on zoom, et que l'on zoom encore et encore. Petit à petit les détails de la photo finiraient par disparaître pour ne laisser apparaitre que des séquences ininterrompues de dégradés ou d'aplat de couleur. C'est le monde analogique, un monde infini, un monde fait d'atomes. Faisons la même expérience avec la même photographie, mais cette fois-ci à travers un écran d'ordinateur. Nous finirons par apercevoir un pixel représentant une couleur. C'est le monde discret, un monde fini, un monde fait de 0 et de 1. Car pour représenter une image sur un écran d'ordinateur, il faut bien réduire l'image à nombre de pixels finis. Lors de la digitalisation, on procède à un échantillonnage afin d'attribuer à l'image un nombre de valeurs finis, et ainsi, la voir sur un écran fait de pixels.

Hormis, l'aspect technique, la digitalisation nous montre deux constats. Le premier constat, si on parle aujourd'hui de raison computationnelle comme l'évoque Bruno Bachimot {cite:p}`bachimont_2004`, c'est parce que l'informatique repose sur des ensembles discrets. Le père de l'informatique, Alan Turing, parle de « machine à états discrets ». C'est cet aspect discret, quantifiable, qui fait de l'ordinateur une machine dont l'essence même est le calcul. Cela nous ramène au deuxième constat, avec le média qu'est l'informatique toutes les informations qui y transites sont reproductibles à l'identique, car quantifiables.


Ces niveaux d'abstraction ne sont pas sans conséquence, car même en maitrisant l'ensemble des différents niveaux de langage. L'informatique se montre à nous seulement dans sa forme la plus riche sémiotiquement, nous ne lisons pas vraiment l'information qui y transite, seulement les signes qu'il nous projette. Ce qui fait de nous plus ou moins, des analphabètes. 

Les médias analogiques respiraient et étaient empreints de leurs protocoles. Quelqu'un face à un livre même écrit en langue étrangère, pouvait voir le protocole émaner du livre, il y avait une forme indiciaire entre la réalité et le médium. Avec Charles Sanders Pierce on distingue trois types de signes {cite:p}`burks_1949` :

- L'indice, la dualité représentant et représenté - un coup frappé à la porte est l'indice d'une visite.
- L'icône, basé sur la ressemblance - une peinture réaliste d'une pomme.
- Le symbole, rapport de convention entre humains - un "Y" inversé symbole de la paix.


Pour résumer, l'indice désigne, l'icône ressemble, le symbole évoque. L'informatique et la culture du logiciel convoque tous ces signes afin que nous puissions créer des opérations herméneutiques. En réalité, les protocoles restent opaques, et nos ordinateurs agissent comme des boîtes noires pour la plupart d'entre nous. Quand on essaye de voir ce qu'il se passe derrière la représentation d'un bits d'informations, on en comprend toute la complexité :

> «*Ce processus repose sur un ensemble dense et complexe de transformation symboliques au termes desquelles un "bit", en tant que valeur binaire stockée dans une mémoire d'ordinateur, est converti en un voltage qui passe à travers la tête de lecture/écriture du disque dur où le courant génère un champ électromagnétique qui renverse la polarité non d'un seul mais de plusieurs dipôles magnétiques - ce qui entraîne des patterns de renversement de flux - implantés dans le substrat matériel du disque. De même, pour lire les données sur la surface du disque, ces patterns de champs magnétiques qui sont reçus comme des signaux analogiques, sont interprétés par le circuit de détection de la tête de lecture comme un saut de voltage, qui est alors converti par l'appareil en une représentation binaire de (1 ou 0) La tête de lecture/écriture mesure des renversements de champs magnétiques plutôt que la charge précise d'un dipôle magnétique individuel. En d'autre termes, il s'agit là d'un appareillage différentiel: la signification dépend d'un changement de la valeur du signal reçu, plutôt que la nature propre du signal lui-même.*»{cite:p}`kirschenbaum_2008`

Le médium informatique est donc un méta-médium, *méta* dans le sens où il englobe tous les autres médiums. Les protocoles liés aux autres médiums sont réinvoqués sous des logiques de digitalisation, des logiques d’échantillonnage. Alors que le vrai protocole est caché à l'individu derrière son écran - l'informatique est une boîte-noire.

Cela nous invite à reconsidérer ce que nous voyons, ce que nous archivons, ce que nous nous remémorons. Lorsque je regarde une peinture du XVIII{sup}`ème` siècle à travers mon écran, lorsque j'écoute un concert de Jimi Hendrix sur YouTube, lorsque que j'écris mon mémoire de fin d'étude par le biais d'un clavier… Toutes ces actions qui sont pour moi des opérations herméneutiques riches de signes, ne sont en réalité que l'écriture de 0 et 1. On fait souvent la critique que nous vivons dans une société d'images, où le texte tend doucement à disparaître et que les jeunes générations n'écrivent plus. La réalité est toute autre, l'écriture n'a jamais été aussi présente, une écriture faite de 0 et 1.

À l'ère du Big Data, à l'ère du tout numérique, à l'ère des 0 et 1, il est essentiel de réapprendre à lire et à écrire si on veut se saisir des enjeux en cours en termes d'archives. C’est ce que nous invite à faire Vilèm Flusser:

> «*Ils sont allés à l’école pour apprendre l’alphabet, mais voilà, les programmes ne sont pas alphabétiques. Or, qu’y a-t-il de plus terrible au monde qu’un texte indéchiffrable ? C’est pourquoi ils craignent le futur.*»{cite:p}`flusser_2006`

## L'algorithme d'une archive dynamique

Les médias ont toujours permis la circulation de l'information et par conséquent du savoir qui a toujours été un lieu d'affrontements. Il existe donc une ambivalence entre localisation et délocalisation autour de l'archive. Lorsqu'un auteur écrit un livre ou bien qu'un scientifique publie un papier sur Internet, ils cherchent à faire circuler leurs messages, lui donner forme - *informationem* - à travers un protocole. Ce protocole permet souvent la production, la reproductibilité et la transmission du message. Mais paradoxalement comme l'explique Olivier Asselin :

> «*La diffusion du savoir sur un vaste territoire suscite un désir, tout à fait fantasmatique, de totalisation qui revient souvent à une sorte de relocalisation. On set met à rêver du livre des livres, comme l'encyclopédie, mais aussi au lieu des lieux, qui pourrait contenir tout le savoir du monde dans une seule architecture, comme dans le Mouseîon d'Alexandrie, qui comprenait, entre autres, une bibliothèque et un musée.*» {cite:p}`asselin_2019`

L'archive numérique n'échappe pas à cette ambivalence. Mais comme nous l'avons souligné précédemment, ce qui change avec l'archive numérique, c'est son protocole et la forme que prend l'information : sa structuration. Avec l'informatique tout est encodable en bits d'informations, en valeurs discrètes. Ce qui fait d'une image, un son, un texte, d'un point de vue conceptuel la même chose. C'est là qu'entre en jeu la raison computationnelle, la raison basée sur le calcul.

J'aimerais maintenant explorer séparément et conjointement les enjeux de l'archive numérique et des données massives. Conscient qu'il existe de nombreux pans de recherches et de sujets autour de ces questions, c'est à travers trois concepts : l'agrégation, la réécriture, le temps critique, que je souhaite développer mon propos. Afin de mettre en lumière ces territoires de totalisation et de relocalisation du pouvoir, mais également leurs conséquences sur la façon dont nous percevons le temps et l'archive. Mais avant tout, une explication sur les relations qu'ils subsistent entre ces trois concepts. 

 L'agrégation, car tout est valeur binaire, calculable et quantifiable. Cette notion, d'agréger de l'information est devenue le socle de l'archive moderne à l'heure des données massives. L'information y est alors regroupée, fractionnée, triée, imbriquée, classée. Le pouvoir dans l'agrégation, c'est la notion de relation - basé sur les liens qui existent au sein d'un ensemble d'informations. On assiste donc à un changement de paradigme, entre le régime classique de l'archive qui était celui de la hiérarchisation, à celui du réseau {cite:p}`gitelam_jackson_2013`. L'archive moderne n'est plus un appel et un retour d'une information qui aurait été stockée, mais plutôt la réactivation de ses relations au sein d'un ensemble.

La réécriture, puisque toute information est calculable, implique également l'idée de réécriture, concept qui au cœur même de l'informatique comme nous l'explique Friedrich Kittler :

> «*Les deux signaux de direction les plus importants qui relient l'unité centrale de traitement de l'ordinateur à la mémoire externe sont appelés READ [lire] et WRITE [écrire].*» {cite:p}`kittler_dirk_1996`

Cela signifie que toute information, tout «savoir», peut-être copié à l'identique. Ce qui entraîne tous les questionnements et débats actuels sur les droits d'auteur, comme l'engouement qu'il y a autour des NTF (*non-fungible token*), mais également autour du piratage. Dans un livre documentaire qui s'intitule MP3 une économie politique de la compression, Jonathan Sterne revient admirablement sur l'histoire passionnante qui lie l'économie autour de l'échantillonnage et propriété intellectuelle {cite:p}`sterne_2018`.

Le temps critique, car c'est celui de la temporalité propre à la machine. Ces notions de synchronisation, streaming, mémoire vive, cache, cycle, réseau, ubiquité coexistent et s’orchestrent à l'intérieur de nos ordinateurs, bouleversent notre rapport au temps, l'espace, et l'archive. Car si pour Marshall McLuhan les nouveaux médias représentaient l'extension des sens de l'être humain {cite:p}`mcluhan_1977` . L'ordinateur représente aujourd'hui notre système nerveux central.

### Métadonnées, standard: préserver l'information et contrôler les signes.

> «*J'ai un rêve pour le Web [dans lequel les ordinateurs] deviennent capables d'analyser tous les données sur le Web — le contenu, les liens et les transactions entre les personnes et les ordinateurs. Un « Web sémantique », qui devrait rendre cela possible, n'a pas encore vu le jour, mais quand c'est le cas, les mécanismes quotidiens du commerce, de la bureaucratie et de notre vie quotidienne seront manipulés par des machines parlant à des machines. Les « agents intelligents » vanté depuis des lustres vont enfin se matérialiser.*» {cite:p}`berners_lee_1999`

> «*Il convient de rappeler que l'archive comme condition de notre connaissance de l'histoire, dépend du support de sa transmission*» {cite:p}`ernst_2012`

Même si l'ensemble des protocoles informatiques se basent sur cette notion de binarisation et d'échange entre machine-machine. Représenter l'information, injecter des signes {cite:p}`burks_1949`, procéder à des opérations herméneutiques {cite:p}`ring_2014`  sont des  pratiques et un aspect foncièrement humain - et nous permettent d'interagir  avec l'ordinateur.

Comme nous l'avons démontré, créer du sens et du savoir, c'est avoir le pouvoir et l'autorité de représenter les choses. Les métadonnées, qui sont aujourd'hui présentes à l'intérieur de l'ensemble du protocole informatique, cachées derrière nos écrans, sont les données qui décrivent des données, une condition *sine qua none* pour la préservation et l'échange à long terme de l'information. Elles représentent également l'étendard d'un nouveau pouvoir mis en place. - « Code is Law » {cite:p}`lessing_2000`

Les institutions culturelles liées à l'archivage de l'information ont toujours été enclin aux enjeux des métadonnées  bien avant l'avènement du numérique, et bien avant le Web Sémantique{cite:p}`berners_lee_1999`. Créer des données à partir d'autres données - les bibliothécaires par exemple ont dû élaborer un ensemble de systèmes de description et de structuration de l'information, en créant des métadonnées. Savoir s' il s'agit d'un auteur, d'une édition, du titre d'un ouvrage, avec l'idée centrale d'indexer des données qui reposent sur d'autres données.

Mais pour échanger des métadonnées il faut se mettre d'accord sur le signifiant et le signifier, sur les indices, sur les symboles {cite:p}`burks_1949`. C'est là qu'entre en jeu la notion de standard. Car comme le rappelle Lisa Gittelman {cite:p}`gitelman_2008` pour qu'un protocole informatique fonctionne il doit avoir une structure socialement réalisée.

Établir des standards, c'est établir la loi ce qui peut-être dit {cite:p}`foucault_1969` , mais également la loi de ce qui peut-être fait. Avec les données massives, de plus en plus de standards veulent être imposés par les entreprises de la Sillicon Valley, des musées, des bibliothèques ou consortiums - comme International Standards Organization ou bien Art and Architecture Thesaurus. Ces standards suivent, la majorité du temps, une approche *top-down*. Créés par un petit groupe d'experts, ils sont ensuite proposés ou imposées au « peuple »..

Pour ce faire, depuis 2014 à l'initiative du Data FAIRport la notion de FAIR *data* ou données FAIR {cite:p}`fair` a émergée dans le but de créer un socle commun en matière métadonnée. FAIR est un acronyme qui repose sur quatre principes : *Findable, Accessible, Interoperable, Reusable.*

- *Findable*, chaque métadonnée doit être facilement retrouvable par une machine ou un humain, notamment par la création d'un identifiant unique et pérenne.
- *Accessible*, chaque métadonnée doit être facilement récupérable par un protocole standard de communication et même quand la donnée n'est plus accessible.
- *Interoperable*,chaque métadonnée doit pouvoir être accessible au sein d'autres environnements et extensible par d'autres utilisateurs.
_ *Reusable*, chaque métadonnée doit fournir leurs cas d'utilisation, leur provenance et doit également s'appuyer sur d'autres standards reposant sur les mêmes principes FAIR

Il existe une convergence entre les principes FAIR et l'*open-source* dans le sens où les deux visent à favoriser l'accessibilité et la réutilisation des ressources numériques. L'*open-source* encourage la transparence et la collaboration en permettant aux utilisateurs d'accéder, de modifier et de distribuer le code source des logiciels. Ils tiennent la promesse {cite:p}`turner_2013` d'un Internet et d'un monde numérique enclin d'un savoir partagé, d'une horizontalité dans ce qui peut-être dit et représenter. L'open-source et le principe FAIR comblent les silences trop longtemps présents au sein de l'archive par des lignes de codes. Mais c'est également aussi la promesse d'une plus grande longévité, loin de l'obsolescence programmée qu'offre certains acteurs. {cite:p}`latouche_2012`. 

Pour se saisir des enjeux liés aux métadonnées en termes de mémoire, d'archive et de savoir, il faut faire le parallèle avec l'archive historique. Dans cette dernière, il y a la notion de préservation d'informations et de la trace autour d'une source - d'un artefact. Il existe plusieurs niveaux hiérarchiques dans la manière de préserver une source.

Le premier niveau est la préservation de l'artefact en lui-même, on parle de préservation de la source primaire. Le deuxième niveau c'est la préservation d'autres artefacts qui traitent de la source primaire; un livre, une photographie, un enregistrement qui viendrait décrire ou évoquer la source primaire. Cette notion d'auto référencement peut continuer d'être incrément; une information qui parle d'une autre information, elle-même traitant d'une autre information à propos d'une source primaire. On retrouve derrière cette idée d'autoréférence, l'idée qui a animé le Web Sémantique de Tim-Berners-Lee {cite:p}`berners_lee_1999`. Mais également l'idée agrégation et de réactivation des relations qui existent entre différentes sources d'informations.

La différence entre la préservation d'une information dans l'archive historique classique, et la notion de préservation d'une information au sein d'un monde fait de 0 et 1, c'est la fragilité de ce deuxième. Une information numérique qui fait référence à un artefact bien tangible, bien matériel, appartenant à un monde analogique, même supprimé, est sans grande conséquence car l'artefact lui-même reste intègre. Mais quand est-il d'une information née numériquement, dîtes *born-digital* ?

Les métadonnées qui ne concernent pas de l'information *born-digital* sont une sorte d'épitaphe du monde réel, elles fournissent une description classique de cette information. Un tableau exposé à l'intérieur d'un musée par exemple, disposerait de métadonnées renseignant ses dimensions, son historique de conservation, son auteur, les techniques employées. Si ces métadonnées venaient de disparaître, cela aurait évidemment des conséquences d'un point vu d'accessibilité et de portabilité de l'information - de savoir - mais non en termes d'oubli, car hormis la disparition de la peinture, l'information - la source primaire - reste liée à son médium - la toile. Mais quand la source primaire de l'information découle du médium numérique; créer des épitaphes n'est pas la solution.

C'est là qu'entre en jeu la notion de réécriture, c'est cette nouvelle dimension de manipulation de l'information qu'offre le média numérique par rapport aux médias analogiques : copier à l'identique. Reprenons l'exemple du tableau, aujourd'hui diverse procédure son mises en place afin de préserver cet artefact, sécurisation aux niveaux de l'accès, contrôle atmosphérique, sécurité électronique, traitement chimique, limitation à l'exposition de la lumière font partie d'une longue liste de procédure créés à des fins de préservation. Car ce qui fait de l'archive, un lieu de pouvoir, ce qui cristallise l'attention de celle-ci, dans sa quête de préservation, c'est sa singularité. L'authenticité du monde analogique n'existe pas dans celui de l'information binaire, tout est discret, tout est copiable.

Préserver une trace numérique n'est donc pas qu'une histoire de métadonnées, c'est aussi une histoire de réécriture. Les données massives, ce sont les données *born-digital*, car l'avènement du Big Data se fait par la production de données nées au sein de nos architectures numériques. Aujourd'hui chaque utilisateur produit du contenu numérique de façon consciente par la création d'artefacts numériques ou de manière inconsciente par sa simple présence sur le réseau. Toutes ces données nourissent une économie dont Shoshana Zuboff explique les origines dans son ouvrage *Le Capitalisme de Surveillance* {cite:p}`zuboff_2020`.

Les entreprises, les institutions qui stockent des données toujours plus importantes ont donc besoin de métadonnées afin de décrire l'ensemble des données qu'elles agrègent. Elles ont également tout intérêt à préserver celle-ci de la fragilité du médium informatique. La réécriture est donc au cœur d'un ensemble de techniques qui repose sur des « systèmes de fichiers distribués ».

Le médium informatique est fragile par définition, même si les composants utilisés pour la construction d'un ordinateur sont à la pointe de l'ingénierie, il offre une longévité beaucoup plus faible que les médias analogiques. Quand on parle de préservation de l'information numérique, le premier constat est donc la défaillance omniprésente du médium qu'elle habite. Pour se faire, une technique consiste à distribuer l'ensemble de l'information - la réécrire - à travers différentes machines. Cela empêche les risques de perdre de l'information, car si une machine ne fonctionne plus, les autres peuvent être garantes de celle-ci. On parle de résilience aux pannes ou aux défaillances du système.

Cette solution offre plusieurs avantages en termes de raison computationnelle. La première, c'est l'extensibilité de calculer l'information. La distribution de l'information à travers un ensemble de matériaux informatiques, s'accompagne d'une distribution du calcul : le parallélisme. Chaque instance de machine peut effectuer des opérations de calcul sur l'ensemble des données qu'elle possède, le tout est ensuite agrégé pour créer l'opération herméneutique. Cette nouvelle façon, de préserver l'information, d'accéder à celle-ci, a eu plusieurs conséquence pour l'archive classique en terme de vélocité, de volume, de véracité et de variété {cite:p}`hrehova_2018`

De nombreuses institutions ont digitalisé les informations et catalogues dont elles disposaient, en se basant sur les métadonnées qu'elles possédaient. Ces métadonnées ne pouvaient pas profiter de la raison computationnelle, car les archives classiques disposent de structures et de protocoles liés à l'essence même des médias analogues. Ces structures sont parfois inconsistantes et difficilement lisibles pour la machine. Dans ce cas, les métadonnées prennent la forme d'une épitaphe, sans réelle vitalité computationnelle. C'est l'une des raisons qui pousse notamment de nombreuses institutions à reparcourir leurs catalogues grâce à des outils qui se basent sur des réseaux de neurones profonds, afin de rendre consultable et lisible pour la machine les informations présentent à l'intérieur des médiums analogiques. Cela permet de construire des métadonnées qui exploitent ainsi pleinement la raison computationnelle. 

Prenons l'exemple de métadonnées qui viendraient décrire l'ensemble d'une collection de manuscrits. Les métadonnées associées à chaque manuscrit disposeraient de la date, de son auteur, d'explication sur son origine ou bien également d'une description de son contenu - c'est la fameuse épitaphe. Puis une fois le manuscrit digitalisé avec l'ensemble des métadonnées, l'utilisateur à la possibilité d'indexer par le biais de ces métadonnées le manuscrit. Cela ressemble beaucoup au rôle du bibliothécaire, car même si la raison computationnelle est présente, elle n'est pas pleinement exploitée.

Maintenant grâce à un modèle d'Intelligence Artificielle qui repose sur des réseaux de neurones profonds. Une institution parcourt l'ensemble de cette collection digitalisée. Cette fois-ci, elle extrait l'ensemble du texte - océrisation - les images présentes - segmentation - les informations présentes dans l'image - détection d'objet - le texte présent dans l'image. Une fois cette chaîne de traitement terminée, elle obtient un ensemble de métadonnées très riche, et grâce à ces métadonnées elle peut profiter pleinement de la raison computationnelle. L'utilisateur peut, cette fois-ci, faire ce qu'un bibliothécaire ne serait pas en mesure d'effectuer. Compter le nombre d’occurrence d'un mot sur pour une période précise, classer les images par dominantes de couleurs ou bien rechercher toutes les enluminures en les classant par similarité. 

Ces méthodes ont déjà lieu dans de nombreuses institutions, sur des initiatives individuelles ou collectives avec la création de standard (DCMI, MARC, AACR2, MODS, TEI, OAI/PMH){cite:p}`balnaves_2013`, et de modèle IA (eScriptorium, Transkribus){cite:p}`escriptorium,transkribus`. Elles mettent à jour les possibilités computationnelles de leurs catalogues, mais cela pose un autre problème, celui du volume.

C'est l'une des raisons pour lesquelles, certaines institutions se montrent réticentes à ouvrir l'accès de leurs catalogues aux moteurs de recherche commerciaux. Notamment, la peur d'être exposé à de trop nombreuses requêtes. Conséquence, de nombreux prestataires comble cette difficulté en proposant des contenus issus de certaines archives sous droit de licence. Car aujourd'hui la plupart des utilisateurs comme des chercheurs, ne recherchent pas forcément une confrontation avec la source primaire, il recherche juste l'information et si possible sous forme digitale dans le confort qu'offre leurs environnements numériques. Ce qui soulève un autre problème, la véracité de l'information.

Comme nous l'avons vu précédemment, dans le monde de l'archive classique, les médias analogues comme la photographie sur pellicule, l'écriture manuscrite ou bien d'une bande magnétique disposent d'un protocole indiciaire. Ce protocole dispose d'une certaine forme de véracité, de singularité et donc d'authenticité. En rappelant que cette véracité n'est pas garantie et peut-être soumise à manipulation, comme nous montre Nathalie Davies dans l'analyse des lettres de rémissions à la Chancellerie royale française de 1523 à 1568 {cite:p}`davis_1990`. La difficulté dans l'établissement de la véracité d'une information digitale est double. Tout d'abord les relations auto référencement peuvent être brisées. Un utilisateur ou chercheur veut avoir la possibilité d'activer les relations de la source digitale qu'il possède, afin d'être en mesure de localiser d'autres sources, qu'elles soient secondaires ou primaires à l'instar d'une note de bas de page qui viendrait faire référence à une autre source d'informations. Cette notion d'auto référencement présente à travers le Web Sémantique notamment à travers des formats de métadonnées de type graphe comme le *Resource Data Framework* (RDF) peuvent être brisées - c'est à dire supprimées. Ensuite, il est presque impossible d'établir une vérité absolue sur la datation ou l'origine d'une information *born-digital*, du fait de son caractère discret. Comment établir que telle publication, sûr tel réseau, ne fait pas l'objet d'une publication différée ? Comment établir que cette photographie numérique à bien été prise à telle date ? C'est toute la chronologie et la spatialité qui font de l'histoire ce qu'elle est, qui est aujourd'hui remise en question.

Avec les données massives pour créer de la valeur, fouiller des données, mettre à jour leurs relations, leurs offrirent des caractéristiques mnémonique en termes de visualisation et de compréhension, il faut être capable d'agréger l'ensemble. L'agrégation se base sur la raison computationnelle, qui elle-même repose sur la vélocité de regrouper, séparer, trier et classer un ensemble d'informations. Le chercheur ou utilisateur veut avoir l'ensemble de ces possibilités à portée de clic. Quand avec l'archive classique, les opérations herméneutiques étaient insufflées par les historiens, les paléographes, les comptables ainsi que d'autres corps de métier, qui formaient entre eux un groupe d'experts, gardien du «savoir». Aujourd'hui, les nouveaux experts sont les ingénieurs de la donnée, les statisticiens, les ingénieurs en apprentissages automatiques, car eux seuls sont en mesure de construire les outils nécessaires afin de manipuler cette raison computationnelle et les protocoles sous adjacents. C'est l'une des raisons qui pousse des chercheurs en lettres comme Franco Morreti, fondateur du *Stanford University Lab* inventeur du *distant reading* {cite:p}`moretti_2008` à s'approprier cette raison du calcul et à l'incorporer au sein de sa discipline. Cette nouvelle manière de voir et d'appréhender l'information est porteuse de promesse en terme de collaboration pour remettre des mots entre les « pages blanches » de l'archive

>*"Nous ne voulons pas dire que c'est ainsi que les historiens « feront » l'histoire lorsqu'il s'agit de mégadonnées ; ce n'est plutôt qu'un élément de la boîte à outils, une façon de plus de traitant de «grosses» quantités de données avec lesquelles les historiens doivent maintenant se débattre. De plus, un « macroscope », un outil pour voir le très grand, suggère délibérément l'établi d'un scientifique, où l'enquêteur se déplace entre différents outils pour explorer différentes échelles, en prenant des notes dans un laboratoire carnet de notes. De même, une approche des données massives pour l'historien (selon nous) doit être une approche publique, l'historien gardant un cahier ouvert afin que d'autres peuvent explorer les mêmes chemins à travers l'information, tout en parvenant éventuellement à des conclusions très différentes. C'est une approche générative : les données pour les sciences humaines ne consistent pas seulement à justifier une histoire sur le passé, mais à générer de nouvelles histoires, de nouvelles perspectives, compte tenu de nos nouveaux points de vue et outils"* {cite:p}`milligan_al_2015`

### Le temps des machines


```{figure} assets/jab33_leroux_002f.jpg
:align: center
Etienne-Jules Marey - Cavalier Arabe, 1887 | Chronophotographie sur plaque fixe | © Musée Marey - Beaune / J.Cl. Couval]
```

> *"Éclaircissons enfin la relation entre ce micro-niveau des pratiques de stockage et la mémoire dite sociale. Tout le monde parle d'augmenter les capacités de «mémoire» informatiques, une vieille obsession occidentale que la culture dépend du stockage matériel (architectures historiques, bibliothèques, musées). Mais l'analyse des médias indique que l'accent culturel futur sera plutôt mis sur le transfert permanent. Il y a déjà une implosion du stockage dans les flux de données processuels [...] La notion permanente de feedback des données remplace la séparation qui faisait traditionnellement toute la différence en matière d'archivage."* {cite:p}`ernst_2012`

Nous percevons les archives classiques, comme quelque chose de statique et de rigide. Aujourd'hui avec le numérique, comme nous l'avons démontré précédemment, l'archive est devenue quelque chose de fluide, dynamique, stockée ici et là, soumis à la latence de nos environnements informatiques, soumis à leurs fragilités. C'est la notion d'incertitude qui règne dans le monde de l'archive numérique. Une incertitude spatiale, une incertitude temporelle.

Dans la culture ouest-occidentale, le temps est le plus souvent vu comme un flux - un flow - qui s'écoule de façon continue. Les progrès techniques et l'apparition des nouveaux médias ont permis à l'être humain de manipuler ce flux temporel. La chrono-technique par exemple a permis de mettre en lumière, ce qui n'était pas perceptible pour l'être humain, en lui révélant ainsi que le temps des machines n'est pas celui de l'homme. Mettre sur pause, rejouer, accélérer, décélérer, avancer, enregistrer, rembobiner, toutes ces techniques ont permis l'avènement d'une nouvelle façon de voir le temps et de le ressentir. La chronophotographie « Le cheval en mouvement » par Eadweard Muybridge, a permis par exemple, de décomposer le galop d'un cheval en une série de photos distinctes, et de montrer par la même occasion qu'il subsiste un court moment où tous les sabots quittent simultanément le sol. Chose que l’être humain n’était pas capable de voir, car il n'est pas en mesure de se rendre compte de ce que Gottfried Wilhelm Leibniz nomme ces « petites perception » {cite:p}`leibniz_1996`.

Le temps critique {cite:p}`moles_1990`, ce sont ces petites perceptions que l'ordinateur effectue, qui sont nécessaires mais imperceptibles à l'être humain. Comme nous l'explique Wolfgang Ernst :

> «*Pour que toutes les valeurs échantillonnées soient calculées en même temps dans une fenêtre temporelle, elles doivent rester temporairement stockées ; l'analyse en temps réel est basée sur la latence dans le présent. L'espace de l'archive et l'actualité du présent ne sont donc plus strictement séparés, mais plutôt une condition mutuelle.*»{cite:p}`ernst_2016`

Dans cette citation de Wolfgang Ernst, le mot « latence » est à mettre en gras. La latence en physique, c'est la vélocité inhérente à une information pour se déplacer dans une dimension spatiale. En informatique, elle signifie le délai de transmission d'une information. Cette notion de latence est importante , car elle rejoint les enjeux que nous avions soulevés précédemment en termes d'accessibilité de l'information et de disponibilité. Tous les échanges informationnels liés à la messagerie instantanée, le *streaming*, le *Cloud*, reposent sur cette notion de latence homme-machine - ces petites perceptions.

L'ordinateur diffère des médias électroniques comme la télévision ou bien la radio, car, contrairement à eux, il ne fait pas juste qu'émettre - un flux informationnelle qui s'écoule de façon continue - mais procède à des micro actions d'archivage à des échelles temporelles imperceptible pour l'homme. Il prend des micro-décisions en termes d'élection, de tri, de séparation de l'information, et rappelle celle-ci dans un présent - il *process* de l'information. 

L'essence même de l'ordinateur se base donc sur des notions d'archivage. On parle alors, de mémoire protégée, de registre (présent également dans l'archive classique) qui sont des emplacements qui mémorise temporairement des résultats, d'accumulateur (registre dédié uniquement aux opérations arithmétiques), de la mémoire tampon qui mémorise les données qui transitent entre les unités fonctionnelles. Toutes ces micro-archives disposent de divers modes de structuration de l'information à l'instar d'une bibliothèque : accès séquentiel, accès indexés séquentiel , empilage et dépilement, accès associatif. Ce qui souligne ce que nous énoncions précédemment, lorsque nous disions que chaque utilisateur à travers son ordinateur se transforme à la fois en archiviste et bibliothécaire. La distinction importante ici, est que ces utilisateurs n'ont parfois pas conscience de ces opérations - c'est la machine qui archive à leur place.

Allumer un ordinateur, c'est donc en quelque sorte commencé à archiver, mais à des vitesses infinitésimales sous des contraintes de latence et d'accessibilité, de temps critique. Ce temps critique, ces petites perceptions nous les ressentons sans en avoir vraiment conscience :

> «*Quel genre de temps passons-nous en ligne, lorsque nous nous perdons parmi des liens morts, que nous sommes bloqués sur des sites à accès restreint, des requêtes de recherche qui mènent à qui sait où? Ce n'est pas l'heure de l'horloge, et ce n'est pas l'heure de Swatch, c'est une période de décalages et de latences, d'attente et de clics, de rapidité et de lenteur. C'est l'expérience de vitesses différentes et d'asynchronicité*»  {cite:p}`lovink_2008`

Les archives ne sont plus seulement collectées, organisées, gérées, murées et conservées par un ensemble d'institutions gardiens de la mémoire, mais se diffusent également à travers une « nouvelle écologie de la mémoire » {cite:p}`brown_hoskins_2010`. En effet, aujourd'hui, l'archive peut même être vue comme un média à part entière, puisqu'elle a été libéré « de l'espace archivistique au temps archivistique » {cite:p}`hoskinks_2017`

Mais cette nouvelle écologie de la mémoire et de l'archive ne pourrait avoir lieu sans Internet et la mise en réseau de ces informations. Aujourd'hui, le territoire qui suscite totalisation et relocalisation c'est Internet, un territoire fait de ponts et de péages. Les institutions classiques qui étaient jusqu'alors des lieux de mémoire clos, se transforment en plateformes, elles créent de nouveaux services à partir de leurs archives, notamment via des *Application Programming Interface* (API), qui permettent d'accéder à leurs données et métadonnées via des requêtes. Et derrière chaque requête se cachent des logiques politiques comme c'est le cas avec la Chine de Xi Jinping, derrière chaque requête se cachent des logiques économiques avec les publicités ciblées, derrière chaque requête se cachent des contraintes matérielles comme avec les "déserts numériques". Il faut donc s'interroger sur l'itinéraire de ces requêtes, sur les logiques qui leurs sont appliquées, car effectuer une requête n'est pas un acte anodin, c'est un acte symbolique, herméneutique, géopolitique et un acte décisionnel. Le courant poststructuralisme a mis en évidence le pouvoir d'exclure de l'archive et cette exclusion passe par l'acte même d'archiver, créant ainsi une forme d'historicité. Aujourd'hui, à l'heure où tout le monde fabrique sa propre histoire, le pouvoir réside dans la capacité à partager son histoire, mais également dans la possibilité de lire celle des autres. C'est ce changement de paradigme qui explique notamment les bulles internet, ce qui met bien en évidence la difficulté à partager une histoire commune.  

Pour comprendre cette nouvelle écologie de la mémoire et pourquoi l'idée d'une accessibilité à l'information en temps réel n'existe pas. Il faut revenir sur la théorie des réseaux de Paul Baran {cite:p}`baran_1964`, le *packet-switching* et le protocole *TCP/IP* en faisant le parallèle avec ce qui nous intéresse en termes d'archives et d’accessibilité à l'information.


```{figure} assets/Centralised-decentralised-and-distributed-network-models-by-Paul-Baran-part-of-a-Rand.png
:align: center
:width: 500px
Diagramme des réseaux par Paul Baran
```

Paul Baran distingue trois types de réseaux :

- Le réseau centralisé où les informations transitent par un nœud central, un accès en temps réel à l'information. Mais la destruction du nœud central entraîne une perte totale d'accès et d'échange à l'information.
- Le réseau décentralisé où les informations transitent par quelques nœuds principaux, il y a une redondance de l'information. Mais la destruction des nœuds principaux entraînerait également une perte totale d'accès et d'échange à l'information.
- Le réseau distribué où tous les nœuds sont des nœuds principaux par lesquels transite l'information, c'est l'Internet d'aujourd'hui. Il y a une très forte redondance de l'information car ce réseau repose sur la réécriture de l'information, la destruction d'un nœud n'engendre pas de grosse perte d'accès et d'échange à l'information.


Internet repose sur cette notion de réseaux distribués. Chaque information qui transite d'un émetteur à un récepteur est divisée en bloc, ce qu'on nomme des «paquets». Ces paquets sont anonymisés sur l'information qu'ils transportent, qu'il s'agisse d'une image, d'une vidéo, d'un texte[^1]. Les seules métadonnées dont disposent ces paquets sont des informations sur l'émetteur, le récepteur ainsi que les différentes procédures pour reconstituer l'ensemble des paquets. Chaque paquet est ensuite distribué sur le réseau, empruntant un itinéraire différent. Lorsqu’un paquet transite par un nœud, celui-ci est interrompu, les métadonnées du paquet sont actualisées avec le temps et l'itinéraire parcouru. C’est là qu’opère le *packet-switching*, il procède à une micro-décision afin de décider du meilleur itinéraire à parcourir en fonction de la charge du réseau. Ces micros-décisions sont au cœur du protocole *TCP/IP*. 

> «*La temporalité des micros -décisions est l'interruption qui se produit au niveau des nœuds à chaque transmissions pour décider de leur direction et de leur priorité. Sans ces décisions, aucune transmissions possible. Le fait que la transmissions soit interrompue en permanence signifie non seulement qu'elle ne peut pas avoir lieu en temps réel, comme on le suppose généralement, mais qu'elle ne relie jamais les êtres humains sans intermédiaire et qu'elle ne permet aucun accès direct au monde [..] La métaphore du flux cache le fait que, d'un point de vue technique, c'est tout le contraire qui se produit. Un réseau n'est pas constitué de flux, le fait que cette métaphore suggère qu'il y aurait un réseau dans un flux est la preuve que, aussi bien dans ce cas que dans la vision d'Internet, la question de transmission et ses détails techniques a été ignorée.*» {cite:p}`sprenger_2017`

Toutes ces explications mettent à jour que l’imaginaire de l'instant, la simultanéité qu'on éprouve lorsqu'on fait une requête pour regarder un film en *streaming*, lorsqu'on parle à un amie via une messagerie instantanée, lorsqu'on parcourt des pages Web, n'existent pas. Ce qu'il faut se demander c'est dans quelle logique politique, économique, matérielle cette information se présente à moi ? Un utilisateur préférera toujours l'accès à une source d'information qui se fait rapidement, la frustration qu'engendre la latence est en partie liée à l'imaginaire d'un Internet relié en direct - d'un flow informationnelle. C'est avec cette nouvelle économie de la latence qu'émerge des formules tarifaires pour être connecté plus vite, pour avoir plus d'accès à l'information, mais quelles sont vraiment les logiques derrière cette requête ? Le résultat que j'obtiens est-il le plus pertinent ? A-t-il été choisi pour moi en fonction des traces que je laisse sur le réseau ? A-t-il été épié ?


Avec l'archive classique, la recherche à l'information était différente. Même si le contrôle était bien présent par la création et l'accessibilité de l'archive, il y avait une dimension temporelle qui n'est plus présente avec l'archive numérique.

> «*Le clic est le geste effectué grâce à la mémoire par habitude, la succession de clics est un saut d'un espace à un autre, dans ces sauts, toutes dimensions temporelle est niée. Tel est le risque, oublier le temp, celui qui passe mais surtout celui nous forme, nous nourrit, un temps qui ne peut-être pensé selon une stricte chronologie des moments qui le composent.*» {cite:p}`truchot_2019`

Avec l'archive numérique on filtre et sélectionne pour nous, parce que la quantité d'informations est beaucoup trop importante. Mais derrière ces opérations herméneutiques se cachent logiques politiques, économiques et matérielles. Il faut donc renouer avec de nouveaux concepts et penser à une esthétique de l'information numérique, ouvrir cette boîte noire à l'utilisateur, lui montrer les mécanismes qui se cache derrière cette immédiateté de l'information, lui offrir la possibilité de parcourir d'autres chemins, de bifurquer. Car c'est dans cette bifurcation que se cache ce que Jean-Pierre Truchot appelle « l’instant fécond », le fait de chercher une chose précise mais d'en découvrir une autre par accident et sagacité, c'est ce qui fait de toute recherche le théâtre de surprise, de plaisir et qui permet de construire sa réflexion. Walpole nomme ça la sérendipité.


Parler d'archive numérique et d'archive classique, c'est sous-entendre les mêmes dynamiques de contrôle et de pouvoir autour de la notion d'information.
De pouvoir, car produire l'archive, c'est construire un récit, créer du sens, du savoir à partir d'une information - la source primaire.
De contrôle, car la production d'information est toujours le théâtre de délocalisations et de relocalisations, à des fins de diffusion ou de captation.
L'avènement du numérique a émancipé l'information de sa source primaire, ce qui a entraîné une production toujours plus importante d'information, mais a également eu des conséquences sur la manière dont nous structurons l'information. Comment créer du sens quand, d'un point de vue binaire, une image ou du texte sont la même chose ? Ces questions sont aujourd'hui toujours ouvertes, même si de grandes initiatives sont en marche depuis plusieurs années, telles que le Web Sémantique, les standards de métadonnées ou encore les réseaux de neurones profonds.
Cette émancipation s'accompagne également d'une nouvelle écologie de la mémoire, car avec le numérique, la notion d'incertitude est omniprésente, une incertitude spatiale et temporelle. Contrairement à l'imaginaire d'un monde numérique plus transparent, l'informatique est avant tout une boîte noire dont les modes d'accès et de fermeture à l'archive sont difficilement compréhensibles. 






 










[^1]: Aujourd'hui et depuis les révélations d'Edward Snowden, de nombreux procédés sont mis en lumière et montrent qu'il est possible d'obtenir des informations sur le contenu des paquets. Notamment avec des techniques liées à la théorie des graphes qui inspectent l'itinéraire de ces paquets ou des outils de type Deep Packet Inspection qui permettent d'avoir des renseignements sur la nature des informations échangées. Ces actions sont seulement possible du fait et par des micro-décisions qui interrompt la transmission.



